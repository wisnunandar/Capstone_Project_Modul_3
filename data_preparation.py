# -*- coding: utf-8 -*-
"""Data Preparation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gvB2ejUbWgSfPbGlXbGw2OXZ3bQRJDnB

# RESUME SCREENING CHATBOT
"""

from google.colab import drive
drive.mount('/content/drive')

#install dependencies
!pip install -q langchain_openai langchain_core langchain_qdrant langgraph
!pip install -qU qdrant-client langchain-openai langchain-qdrant
!pip install -q langchain langchain-openai langchain-qdrant qdrant-client pandas openai tqdm

#import libraries needed
import pandas as pd

from langchain_openai import ChatOpenAI, OpenAIEmbeddings #embedding and LLM services from OpenAI
from langchain_qdrant import QdrantVectorStore #for langchain - qdrant vector database connector
from langchain.tools import tool
from langgraph.prebuilt import create_react_agent
from langchain_core.messages import ToolMessage

from langchain_core.documents import Document #for document format that stored to vector database collection
from qdrant_client import QdrantClient #for qdrant client set up
from qdrant_client.http.models import Distance, VectorParams #for distance method and configuration of vector parameters

from dotenv import load_dotenv

# importing all libraries needed
import getpass, os


from langchain_core.documents import Document #for document format that stored to vector database collection
from qdrant_client import QdrantClient #for qdrant client set up
from qdrant_client.http.models import Distance, VectorParams #for distance method and configuration of vector parameters


from tqdm import tqdm
from langchain_qdrant import QdrantVectorStore
from langchain.docstore.document import Document
import re

#set up and define secret key
from google.colab import userdata

OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')
QDRANT_API_KEY=userdata.get('QDRANT_API_KEY')
QDRANT_URL=userdata.get('QDRANT_URL')

"""## Exploring Dataset"""

df = pd.read_csv('/content/drive/MyDrive/Capstone_3/Resume.csv')
print('data shape :', df.shape)
df.head(20)

"""## Data Preprocessing"""

#duplikasi dataset
data = df.copy()

#missing value checking
for col in data.columns :
  print(f'Jumlah missing value pada kolom {col} sebanyak :', data[f'{col}'].isna().sum())

#Data duplicate checking for ID number feature only
data["ID"].duplicated().sum()

#Involving first 1242 data rows only
data = data[:1242] #hanya melibatkan 1000 baris pertama dari dataset untuk penyederhanaan
print(f'kategori job yang tersedia di dalam dataset setelah filtering :')
for kategori in data['Category'].unique():
  print(kategori)

data.head(20)

"""## Data Cleaning"""

#HTML tags on resume_str column checking
html_rows = data[data["Resume_str"].str.contains(r"<[^>]+>", regex=True, na=False)]
print(f"{len(html_rows)} baris mengandung HTML tags pada baris ke {html_rows.index.tolist()}.")

#Special character checking
import re
def has_unwanted_chars(text):
    return bool(re.search(r"[\n\r\t]", str(text)))

unclean_rows = data[data["Resume_str"].apply(has_unwanted_chars)]
print(f"{len(unclean_rows)} baris dengan newline/tab characters.")

#missing text checking
missing_rows = data[data["Resume_str"].isna() | (data["Resume_str"].str.strip() == "")]
print(f"{len(missing_rows)} empty resumes ditemukan pada baris ke {missing_rows.index.tolist()}")

# Drop rows where resume_str is missing or empty
data = data[~(data["Resume_str"].isna() | (data["Resume_str"].str.strip() == ""))]

# Reset index
data = data.reset_index(drop=True)

print(f"Resume_str yang kosong telah dihapus dan jumlah baris saat ini sebanyak {len(data)}.")

#Data cleaning from special chars
def clean_text(text):
    if pd.isna(text):
        return ""
    text = re.sub(r'<[^>]+>', ' ', str(text))  # remove HTML tags
    text = re.sub(r'\s+', ' ', text)           # remove excessive spaces
    return text.strip()

data["clean_resume"] = data["Resume_str"].apply(clean_text)

#text chunking
def chunk_text(text, chunk_size=500):
    words = text.split()
    return [" ".join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]

documents = []

for i in tqdm(range(len(data))):
    chunks = chunk_text(data["clean_resume"][i])
    for idx, chunk in enumerate(chunks):
        doc = Document(
            page_content=chunk,
            metadata={
                "id": str(data["ID"][i]),
                "category": str(data["Category"][i]),
                "html": data["Resume_html"][i],
                "chunk_id": idx
            }
        )
        documents.append(doc)

print(f"✅ Total chunks created: {len(documents)}")

#missing text checking after cleaning
missing_rows = data[data["clean_resume"].isna() | (data["clean_resume"].str.strip() == "")]
print(f"{len(missing_rows)} empty resumes ditemukan pada baris ke {missing_rows.index.tolist()}")

#missing text checking after cleaning
unclean_rows = data[data["clean_resume"].apply(has_unwanted_chars)]
print(f"{len(unclean_rows)} baris dengan newline/tab characters.")

html_rows = data[data["clean_resume"].str.contains(r"<[^>]+>", regex=True, na=False)]
print(f"{len(html_rows)} baris mengandung HTML tags.")

"""## Text Embedding"""

#create text embedding
embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small",
    api_key=OPENAI_API_KEY
)

# Create documents for Qdrant
documents = []
for i, row in data.iterrows():
    doc = Document(
        page_content=row["clean_resume"],    # cleaned resume text
        metadata={
            "id": str(row["ID"]),
            "category": str(row["Category"]),
            "html": str(row["Resume_html"]) if "Resume_html" in row and pd.notna(row["Resume_html"]) else ""
        }
    )
    documents.append(doc)

print(f"✅ Created {len(documents)} documents ready for embedding.")

documents[0]

#save document to qdrant

collection_name = "newresume_collection"
qdrant = QdrantVectorStore.from_documents(
    documents=documents,
    embedding=embeddings,
    url=QDRANT_URL,
    api_key=QDRANT_API_KEY,
    collection_name=collection_name,
    prefer_grpc=False,
)
print("Resumes successfully embedded and uploaded to Qdrant!")

#indexing
from qdrant_client.models import PayloadSchemaType

client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)

# Create index untuk metadata.id
client.create_payload_index(
    collection_name="newresume_collection",
    field_name="metadata.id",
    field_schema=PayloadSchemaType.KEYWORD
)

print("✅ Index created for metadata.id")

#setup qdrant client
client = QdrantClient(
  url= QDRANT_URL,
  api_key = QDRANT_API_KEY
)
#Check all collections in Qdrant
collections_response = client.get_collections()
print("Collections:", collections_response.collections)

"""## Test Retrieval"""

#Choose collection in Qdrant
qdrant = QdrantVectorStore.from_existing_collection(
    embedding=embeddings,
    collection_name="newresume_collection",
    url= QDRANT_URL,
    api_key=QDRANT_API_KEY
)

#retrieve data
results = qdrant.similarity_search(
    "Candidate who has experience as a Management Trainee and handling 5+ projects",
    k=5
)
results
#parsing page content and metadata from retrieved data
for res in results:
    print(f"Result Page Content: {res.page_content}\nResult Metadata: [{res.metadata}]")
    print("-----"*20)

Sample beberapa dokumen untuk lihat format ID
sample_results = qdrant.similarity_search("", k=5)

for i, result in enumerate(sample_results):
    print(f"\nSample {i+1}:")
    print(f"  ID type: {type(result.metadata.get('id'))}")
    print(f"  ID value: {result.metadata.get('id')}")
    print(f"  Category: {result.metadata.get('category')}")

# kandidat untuk posisi tertentu
position = "HR Manager human resource"

results = qdrant.similarity_search(position, k=10)

print(f"=== Top 10 Candidates for: {position} ===\n")
for i, result in enumerate(results, 1):
    candidate_id = result.metadata.get("id")
    category = result.metadata.get("category")
    preview = result.page_content[:150]
    print(f"{i}. ID: {candidate_id}")
    print(f"   Category: {category}")
    print(f"   Preview: {preview}...\n")